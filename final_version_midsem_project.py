# -*- coding: utf-8 -*-
"""Final Version Midsem Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dbUajiBJPca-AzuUCH-F3bvnDnrRCKHu
"""

# Data manipulation and analysis
import pandas as pd
import numpy as np

# Machine learning libraries
import sklearn as sk
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from xgboost import XGBRegressor

# Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Additional Libraries
from google.colab import drive

# Mounts the Google Drive to the specified directory '/content/drive'
# This allows access to files and data stored in your Google Drive within the Colab environment
drive.mount('/content/drive')

"""***DATA COLLECTION***"""

# Reasa CSV file into a Pandas DataFrame
# The file 'player_21.csv' is located in the specified Google Drive directory and is loaded into the variabe 'first_dataset' for further data analysis
first_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/players_21 (1).csv')

# Create a new Pandas DataFrame 'df1' basedon an existing DataFrame 'first_dataset'
df1 = pd.DataFrame(first_dataset)

"""***DATA PREPROCESSING***"""

# Access and display the column labels (column names) of Pandas DataFrame
df1.columns

# Seelct columns with numeric data typesin the Pandas Dataframe
# This filters and returns ONLY the columns that contain numerical data
df1.select_dtypes(include='number')

# The 'verbose = True' option provides detailed information, including the datatypes,non-null counts, and memory usage
df1.info(verbose=True)

# This provides a previewof the top 20 rows of the DataFrame for us to examine
df1.head(20)

"""*Removing columns with **30%** or more null values*"""

# Defines a threshold for missing data
threshold = 0.3

# Calculates the proportion of missing data for each column
nullpercent = df1.isnull().mean()

# Filter the DataFame to retain columns wit missing data proportions less than the specified threshold
df1 = df1.loc[:,nullpercent<threshold]

df1.info(verbose=True)

# This filters and returns ONLY the columns that DO NOT contain numeric data, excluding integers, floats, etc.
df1.select_dtypes(exclude=['number'])

# Select columns with numeric data types
num1=df1.select_dtypes(include=['number'])

# Select columns with non-numeric
obj1=df1.select_dtypes(exclude=['number'])

# This includes statistics like count, mean, standard deviation, minimum, and quartiles for each numeric columns
num1.describe()

# Import the SimpleImputer classfrom scikit-learn to handle missing values
from sklearn.impute import SimpleImputer

# Create a SimpleImputer instance with the strategyof imputing missing values using the mean
imp=SimpleImputer(strategy="mean")

# Calculate the mean values of the numeric columns in the DataFrame and store them as an array
num1.mean().values

# Fit imputer to the data in 'num1' to replace missing valued with the calculated means
imp.fit(num1)

# Use the imputer to fill missing values in the DataFrame 'num1' and store the result in 'X'
X=imp.transform(num1)

# Create a new DataFrame'num1_imputed' with values, preserving column names
num1_imputed=pd.DataFrame(X,columns=num1.columns)

num1 = num1_imputed

num1.isnull().any()

# Count the number of null values
num1.isnull().sum()

# Forward-fill (ffill) propagates the last observed non-null value forward in each column
new_obj1= obj1.ffill()

new_obj1.info(verbose=True)

new_obj1.isnull().sum()

new_obj1.isnull().sum()

obj1 = new_obj1

obj1.isnull().any()

print(obj1.columns)

# Import necessary preprocessing tools from scikit-learn
from sklearn.preprocessing import LabelEncoder,StandardScaler

#List of column names to be encoded
columns=['player_url', 'short_name', 'long_name', 'player_positions', 'dob',
       'club_name', 'league_name', 'club_position', 'club_joined',
       'nationality_name', 'preferred_foot', 'work_rate', 'body_type',
       'real_face', 'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam',
       'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm',
       'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk', 'player_face_url',
       'club_logo_url', 'club_flag_url', 'nation_flag_url']

# Create a dictonary to store LabelEncoder instances for each colmun
dict_obj = {}

# Iterate through the list of columns and apply LabelEncoder to each
for col in columns:
  obj=LabelEncoder()
  obj1[col]=obj.fit_transform(obj1[col])
dict_obj[col]=obj

obj1

# The resulting DataFrame contains both encoded categorical and numeric data
cleaned_df1= pd.concat([obj1,num1],axis=1)

cleaned_df1.isnull().sum().sum()

print(cleaned_df1.columns)

"""***Feature Selection and Engineering***"""

# Calculate the correlation matrix for the Dataframe
correlation_matrix = cleaned_df1.corr()

# Extract the 'overall' column as the target variable
target= cleaned_df1['overall']

# Sort and store the features by their correlation with the 'overall' tagreget variable in descending order
top_features = correlation_matrix['overall'].sort_values(ascending = False)

# Create a bar plot to visualize the correlation of the featues with the target variable
top_features.plot(kind='bar')

# Set the title for the plot
plt.title("Correlations with the TargetVariable")

# Label the x-axis with "Correlation Coefficient"
plt.xlabel("Correlation Coefficient")

# Display the plot
plt.show()

# Set a correlation threshold limit of 0.55
limits = 0.55

# Extract the column names with the correlation greater than 0.55 with the 'overall' target variable
corrs = correlation_matrix[correlation_matrix['overall']>limits].index

# Create a newDataFrame containing the columns that meet the correlation threshold
chosen_correlations = cleaned_df1[corrs]

#Get the column names
chosen_correlations.columns

# Assigns
features = chosen_correlations

features.columns

"""*Data Scaling and Splitting*

"""

# Remove the 'overall' column from the DataFrame 'features'to create the feature matrix 'X'
X = features.drop("overall",axis=1)

# Extract the 'overall'colum as the target variable 'y'
y = features["overall"]

# Create a StandardScaler instance to standardize the feature matrx 'X'
scaler = StandardScaler()

# Standardize the feature matrix 'X'
X_scaled = scaler.fit_transform(X)
# X_test_scaled = scaler.transform(X_test)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train

X_test

"""***Building and Training Models***"""

# Random Forest Regressor
np.random.seed(42)
forest_model = RandomForestRegressor()
forest_model.fit(X_train,y_train)
forest_model.score(X_test,y_test)

y_pred=forest_model.predict(X_test)
mean_absolute_error(y_pred,y_test)

RF_crosval_score= -np.mean(cross_val_score(forest_model,X,y,cv=5))
RF_crosval_score

# Gradient Boost Regressor
np.random.seed(42)
gradient_model = GradientBoostingRegressor()
gradient_model.fit(X_train,y_train)
gradient_model.score(X_test,y_test)

GB_crosval_score= -np.mean(cross_val_score(gradient_model,X,y,cv=5,scoring='neg_mean_absolute_error'))

GB_crosval_score

y_pred=gradient_model.predict(X_test)
mean_absolute_error(y_pred,y_test)

# XGBoostRegressor
np.random.seed(42)
xgb_model = XGBRegressor()
xgb_model.fit(X_train,y_train)
xgb_model.score(X_test,y_test)

y_pred=xgb_model.predict(X_test)
mean_absolute_error(y_pred,y_test)

XGB_crosval_score= -np.mean(cross_val_score(xgb_model,X,y,cv=5))
XGB_crosval_score

"""***EVALUATION / OPTIMIZATION***"""

# Hyperparameter Tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 25],
}

# Hyperparameter tuning for Random Forest
rf_grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring ='neg_mean_absolute_error')
rf_grid_search.fit(X, y)
print("Random Forest - Best Parameters:", rf_grid_search.best_params_)

# Hyperparameter tuning for XGBoost
xgb_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5],
}
xgb_grid_search = GridSearchCV(XGBRegressor(), xgb_param_grid, cv=5, scoring='neg_mean_absolute_error')
xgb_grid_search.fit(X, y)
print("XGBoost - Best Parameters:", xgb_grid_search.best_params_)

# Hyperparameter tuning for Gradient Boosting
gb_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5],
    # Add other Gradient Boosting hyperparameters
}
gb_grid_search = GridSearchCV(GradientBoostingRegressor(), gb_param_grid, cv=5, scoring='neg_mean_absolute_error')
gb_grid_search.fit(X, y)
print("Gradient Boosting - Best Parameters:", gb_grid_search.best_params_)

"""***TESTING AGAIN***"""

y_pred=xgb_grid_search.predict(X_test)
mean_absolute_error(y_test,y_pred)

y_pred=gb_grid_search.predict(X_test)
mean_absolute_error(y_test,y_pred)

y_pred=rf_grid_search.predict(X_test)
mean_absolute_error(y_test,y_pred)

"""***TESTING WITH NEW DATASET - player_22***

***DATA COLLECTION***
"""

second_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/players_22.csv')

second_dataframe = pd.DataFrame(second_dataset)

chosen_columns=['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',
       'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',
       'overall', 'potential', 'value_eur', 'wage_eur', 'release_clause_eur',
       'passing', 'dribbling', 'movement_reactions', 'power_shot_power',
       'mentality_composure']

second_dataframe=second_dataframe[chosen_columns]

second_dataframe.columns

second_dataframe.select_dtypes(include='number')

second_dataframe.info(verbose = True)

"""*Removing columns with **30%** or more null values*"""

second_dataframe.info(verbose = True)

"""***DATA PREPROCESSING***"""

second_dataframe.select_dtypes(exclude=['number'])

num2 = second_dataframe.select_dtypes(include=['number'])
obj2 = second_dataframe.select_dtypes(exclude=['number'])

num2.describe()

imputer2 = SimpleImputer(strategy = "mean")
num2.mean().values
imputer2.fit(num2)

# Filling the missing values
X = imputer2.transform(num2)
num2_imputed = pd.DataFrame(X, columns = num2.columns)

num2 = num2_imputed

num2.isnull().any()

num2.isnull().sum()

new_object2 = obj2.ffill()

new_object2.info(verbose = True)

new_object2.isnull().sum()

object2 = new_object2

object2.isnull().any()

object2.columns

columns2 =['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',
       'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb']

dict_object2 = {}
for col in columns2:
  object = LabelEncoder()
  object2[col] = object.fit_transform(object2[col])
dict_object2[col] = object

cleaned_second_dataframe = pd.concat([object2,num2], axis = 1)

cleaned_second_dataframe.isnull().sum().sum()

print(cleaned_second_dataframe)

X = features.drop(['overall'], axis = 1)
y = features['overall']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled=pd.DataFrame(X,columns=X.columns)
X_scaled.head()

X

"""***MODEL TRAINING***"""

y_pred2=rf_grid_search.predict(X_scaled)

mean_absolute_error(y,y_pred2)

"""Saving the model"""

import pickle
finalfile=open('trained_model.pkl','wb')
pickle.dump(rf_grid_search,finalfile)